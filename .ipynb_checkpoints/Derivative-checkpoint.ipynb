{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3a8c302-ffe5-4638-bc95-bacc35d1df76",
   "metadata": {},
   "source": [
    "# Numerical differentiation of unequally sampled data using Machine Learning\n",
    "- _Pascal Tribel - Machine Learning Group_\n",
    "- _December 2024_\n",
    "## Abstract\n",
    "Derivative estimation is a fundamental problem in numerical analysis, scientific computing, and machine learning, with applications ranging from solving differential equations to data analysis in physics and engineering. Classical approaches to derivative estimation include finite difference methods, spline-based approximations, and spectral methods. These techniques, while effective in many settings, often face limitations when dealing with irregularly spaced (non-evenly discretized) input data, noise, or highly nonlinear functions.\n",
    "\n",
    "Neural networks and neural operators have recently emerged as powerful tools for learning complex functional mappings. Neural operators, in particular, extend traditional neural network frameworks by operating in infinite-dimensional function spaces, making them highly suitable for learning mappings between functions. This framework has been successfully applied to problems such as solving partial differential equations (PDEs) (e.g., Fourier neural operators) and emulating physical systems. However, the application of neural operators for derivative estimation has not been thoroughly explored, especially for irregularly spaced input intervals.\n",
    "\n",
    "Since the differentiation is commonly formulated using the operator paradigm, but the task can also be defined in the functional paradigm when facing sampled data, we define a framework to compare the performances of different Machine Learning architecture, while comparing them to classical numerical solvers.\n",
    "## Problem statement\n",
    "### Introduction and analytical method\n",
    "In the analytical formulation, the definition of the differential operator of an univariate function is trivial:\n",
    "$$\\frac{df(x)}{dx} = \\lim_{\\delta x\\rightarrow 0}\\frac{f(x+\\delta x) - f(x)}{\\delta x} $$\n",
    "This can in many cases easily be computed if the analytical form of $f(x)$ is known. Once the derivative $f'(x) = \\frac{df(x)}{dx}$ is defined, it can be queried at any point. The value obtained denotes the instant slope of the initial function. Therefore, two formalisms can be used: the _operator_ $\\frac{d}{dx}$ defines a mapping between two functions spaces, but there also may exists a function $g(x, \\hat{f}) = f'(x)$ which computes the mapping between $x$ and the value of the derivative $f'(x)$ given numerical additional information on $f$ denoted by $\\hat{f}$. This second formalism defines a mapping between two vector spaces.\n",
    "### Numerical approaches\n",
    "However, when dealing with sampled data (e.g. from real-world measurements), this analytical method does not work. Multiple approaches have been proposed, including the (2) forward, (3) central and (4) backward difference methods, such as, given three sampled points $x_0, x_1, x_2$ such that $x_0<x_1<x_2$:\n",
    "$$ f'(x_1) = \\frac{f(x_2) - f(x_1)}{x_2 - x_1}$$\n",
    "$$ f'(x_1) = \\frac{f(x_2) - f(x_0)}{x_2 - x_0}$$\n",
    "$$ f'(x_1) = \\frac{f(x_1) - f(x_0)}{x_1 - x_0}$$\n",
    "Only the central difference method can be used in order to compute the derivative of a point $\\hat{x}$ for which the function has not been sampled, by querying the closest left and right point to $\\hat{x}$.\n",
    "Such an approach often suffers when the data sampling is unequally spaced. To solve this problem, it is possible to proceed in three steps:\n",
    "- Interpolate a spline between each k-uplet of neighboring points\n",
    "- Compute the derivative of this spline\n",
    "- Query the value of this derivative for any point in the interval bounded by the $k$ points\n",
    "This approach may suffer by two means: either if the sampling resolution is not sufficient, or if the spline order does not correctly fits (overfits or underfits) the non-linearity of the data.\n",
    "### Experimental approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e51053-378c-4b5c-88ec-017c5943f737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import neuralop\n",
    "from neuralop.models import FNO1d\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import symengine as se\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy\n",
    "from joblib import Parallel, delayed\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3328178f-3065-432e-9ba0-f1f7339563f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(x, y):\n",
    "    return np.mean(np.abs(x-y))\n",
    "\n",
    "def diff_fwd(y, x): \n",
    "    return np.diff(y)/np.diff(x)\n",
    "\n",
    "def diff_central(y, x):\n",
    "    return np.array(np.gradient(y))[:-1]/np.array(np.gradient(x))[:-1]\n",
    "\n",
    "def diff_interpolate(y, x, q):\n",
    "    if len(q.shape)==1:\n",
    "        return scipy.interpolate.UnivariateSpline(np.array(x), np.array(y), k=5).derivative()(np.array(q))[:-1]\n",
    "    else:\n",
    "        return np.array([scipy.interpolate.UnivariateSpline(np.array(x[i]), np.array(y[i]), k=5).derivative()(np.array(q[i]))[:-1] for i in range(q.shape[0])])\n",
    "        \n",
    "def random_transform(f, x):\n",
    "    return f((10*float(np.random.random() - 0.5)) * x + (float(np.random.random() - 0.5))) + (float(np.random.random() - 0.5))\n",
    "\n",
    "def random_polynom(x):\n",
    "    degree = np.random.randint(2, 3)\n",
    "    expression = (float(np.random.random() - 0.5)) * x**degree\n",
    "    degree -= 1\n",
    "    while degree >= 0:\n",
    "        expression += 0.01*(float(np.random.random() - 0.5)) * x**degree\n",
    "        degree -= 1\n",
    "    return expression\n",
    "\n",
    "def random_sum(l):\n",
    "    res = []\n",
    "    for f1 in l:\n",
    "        res.append((float(np.random.random() - 0.5)) * f1 + (float(np.random.random() - 0.5)) * np.random.choice(l))\n",
    "    return res\n",
    "\n",
    "def random_product(l):\n",
    "    res = []\n",
    "    for f1 in l:\n",
    "        res.append((float(np.random.random() - 0.5)) * f1 * (float(np.random.random() - 0.5)) * np.random.choice(l))\n",
    "    return res\n",
    "\n",
    "def random_function(x, input_num_points, output_num_points, input_length, output_length, noise=1e-2):\n",
    "    maximum = 10\n",
    "    minimum = -10\n",
    "    while maximum >= 1 or minimum <= -1 or np.abs(np.max(f_eval)-np.min(f_eval)) <= 10e-2:\n",
    "        bases = np.random.choice([random_transform(se.sin, x),\n",
    "                                  random_transform(se.cos, x),\n",
    "                                  random_transform(se.tanh, x),\n",
    "                                  random_polynom(x)\n",
    "                                 ], size=np.random.randint(1, 5))\n",
    "        f = bases[0]\n",
    "        for i in range(1, len(bases)):\n",
    "            p = np.random.random()\n",
    "            if p < 0.5:\n",
    "                f = 0.1*random_sum([f, bases[i]])[0]\n",
    "            else:\n",
    "                f = 0.1*random_product([f, bases[i]])[0]\n",
    "        if np.random.random() < 0.5:\n",
    "            f = f * se.sin(25*float(np.random.random() - 0.5) * x)\n",
    "        if np.random.random() < 0.5:\n",
    "            f = f * se.cos(25*float(np.random.random() - 0.5) * x)\n",
    "        if np.random.random() < 0.5:\n",
    "            f = f * (se.tanh(x + float(np.random.random() - 0.5)) - 0.5) * float(np.random.random() - 0.5)\n",
    "        df = se.diff(f, x)\n",
    "        \n",
    "        start = np.random.random()-1\n",
    "        end = start + length\n",
    "        input_points = np.sort(np.random.uniform(start, end, size=input_num_points))\n",
    "        output_points = np.sort(np.random.uniform(start, end, size=output_num_points))\n",
    "        f_eval = np.array([float(f.subs({x: p})) for p in input_points])\n",
    "        \n",
    "        df_eval = np.array([float(df.subs({x: p})) for p in output_points])\n",
    "        maximum = np.max([np.max(f_eval), np.max(df_eval)])\n",
    "        minimum = np.min([np.min(f_eval), np.min(df_eval)])\n",
    "    return f, df, input_points, output_points, f_eval + np.random.normal(0, noise, size=input_num_points), df_eval\n",
    "\n",
    "def generate_sample(x, input_num_points, output_num_points, length, noise):\n",
    "    f, df, i_p, o_p, f_e, df_e = random_function(x, input_num_points, output_num_points, length, noise)\n",
    "    return f, df, i_p, o_p, f_e, df_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a270d2-e3bd-4108-9fe3-75e01e2a1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = int(2**15)\n",
    "length = 2\n",
    "input_num_points = 64\n",
    "output_num_points = 64\n",
    "noise = 25e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b77dac3-7457-464f-ac99-cbe7e60ba23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATING = False\n",
    "\n",
    "if GENERATING:\n",
    "    print(\"Generating\", total_size, \"samples\")\n",
    "    x = se.Symbol('x')\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(delayed(generate_sample)(x, input_num_points, output_num_points, length, noise) for _ in tqdm(range(total_size)))\n",
    "\n",
    "    functions, derivatives, input_points, output_points, functions_eval, derivatives_eval = zip(*results)\n",
    "\n",
    "    X = np.array(list(zip(functions_eval, input_points)))\n",
    "    Y = np.array(list(zip(derivatives_eval, output_points)))\n",
    "    np.save(\"X.npy\", X)\n",
    "    np.save(\"Y.npy\", Y)\n",
    "else:\n",
    "    X, Y = np.load(\"X.npy\"), np.load(\"Y.npy\")\n",
    "    total_size, _, num_points = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c97f541f-dab3-43aa-ab0f-21e90a5770ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(X.shape[0]*0.5)\n",
    "val_size = int(X.shape[0]*0.25)\n",
    "test_size = X.shape[0] - train_size - val_size\n",
    "\n",
    "x_train, y_train = torch.tensor(X[:train_size], dtype=torch.float32).to('mps'), torch.tensor(Y[:train_size], dtype=torch.float32).to('mps')\n",
    "x_val, y_val = torch.tensor(X[train_size:train_size+val_size], dtype=torch.float32).to('mps'), torch.tensor(Y[train_size:train_size+val_size], dtype=torch.float32).to('mps')\n",
    "x_test, y_test = torch.tensor(X[train_size+val_size:], dtype=torch.float32).to('mps'), torch.tensor(Y[train_size+val_size:], dtype=torch.float32).to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "860192f8-84af-4bd3-ac89-2daadfd0642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi_0 = x_train[:, 0].max()\n",
    "mini_0 = x_train[:, 0].min()\n",
    "maxi_1 = x_train[:, 1].max()\n",
    "mini_1 = x_train[:, 1].min()\n",
    "\n",
    "x_train[:, 0] = (x_train[:, 0]-mini_0)/(maxi_0-mini_0)\n",
    "x_train[:, 1] = (x_train[:, 1]-mini_1)/(maxi_1-mini_1)\n",
    "\n",
    "x_val[:, 0] = (x_val[:, 0]-mini_0)/(maxi_0-mini_0)\n",
    "x_val[:, 1] = (x_val[:, 1]-mini_1)/(maxi_1-mini_1)\n",
    "\n",
    "x_test[:, 0] = (x_test[:, 0]-mini_0)/(maxi_0-mini_0)\n",
    "x_test[:, 1] = (x_test[:, 1]-mini_1)/(maxi_1-mini_1)\n",
    "\n",
    "maxi_0_y = y_train[:, 0].max()\n",
    "mini_0_y = y_train[:, 0].min()\n",
    "maxi_1_y = y_train[:, 1].max()\n",
    "mini_1_y = y_train[:, 1].min()\n",
    "\n",
    "y_train[:, 0] = (y_train[:, 0]-mini_0_y)/(maxi_0_y-mini_0_y)\n",
    "y_train[:, 1] = (y_train[:, 1]-mini_1_y)/(maxi_1_y-mini_1_y)\n",
    "\n",
    "y_val[:, 0] = (y_val[:, 0]-mini_0_y)/(maxi_0_y-mini_0_y)\n",
    "y_val[:, 1] = (y_val[:, 1]-mini_1_y)/(maxi_1_y-mini_1_y)\n",
    "\n",
    "y_test[:, 0] = (y_test[:, 0]-mini_0_y)/(maxi_0_y-mini_0_y)\n",
    "y_test[:, 1] = (y_test[:, 1]-mini_1_y)/(maxi_1_y-mini_1_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8be3afc7-0af6-4a44-ba61-21aa1cb175da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(input_shape, 512)\n",
    "        self.l2 = nn.Linear(512, 256)\n",
    "        self.l3 = nn.Linear(256, 128)\n",
    "        self.l4 = nn.Linear(128, output_shape)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs, query):\n",
    "        x = self.relu(self.l1(torch.concat([inputs.reshape((inputs.shape[0], -1)), query.reshape((query.shape[0], -1))], axis=1)))\n",
    "        x = self.relu(self.l2(x))\n",
    "        x = self.relu(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * input_shape//4, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, output_shape)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs, query):\n",
    "        x = self.relu(self.conv1(torch.concat([inputs.reshape((inputs.shape[0], 1, -1)), query.reshape((query.shape[0], 1, -1))], axis=2)))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class NeuralOperator(nn.Module):\n",
    "    def __init__(self, n_modes_height=64, n_layers=1, hidden_channels=16):\n",
    "        super(NeuralOperator, self).__init__()\n",
    "        self.fc = neuralop.models.FNO1d(n_modes_height=n_modes_height, n_layers=n_layers, in_channels=3, out_channels=1, hidden_channels=hidden_channels)\n",
    "\n",
    "    def forward(self, inputs, query):\n",
    "        inputs = inputs.view(inputs.size(0), -1, query.size(-1))\n",
    "        return self.fc(torch.concat([inputs, query.unsqueeze(1)], 1)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "845f55a1-9da3-4311-8aea-225c97fbf3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network: 1920 parameters\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "neural_network = NeuralNetwork(input_num_points*2+output_num_points, output_num_points).to('mps')\n",
    "print(\"Neural Network:\", np.sum([len(i) for i in neural_network.parameters()]), \"parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6b2b8ba-030f-4eb3-98ac-821117b64289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Neural Network: 1888 parameters\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "cnn = CNN(input_num_points*2+output_num_points, output_num_points).to('mps')\n",
    "print(\"Convolutional Neural Network:\", np.sum([len(i) for i in cnn.parameters()]), \"parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "822da17d-b352-40d7-b604-dadd2ef1c725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Operator: 1670 parameters\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "neural_operator = NeuralOperator(128, 4, 64).to('mps')\n",
    "print(\"Neural Operator:\", np.sum([len(i) for i in neural_operator.parameters()]), \"parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8b2ecb2-88e4-45b7-8245-7fe643155a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs, batch_size):\n",
    "    best_model = deepcopy(model)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    val_losses = []\n",
    "    best_loss = np.inf\n",
    "    for epoch in (pbar := tqdm(range(n_epochs))):\n",
    "        model.train()\n",
    "        for b in range(0, x_train.shape[0], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(x_train[b:b+batch_size], y_train[b:b+batch_size, 1])\n",
    "            loss = criterion(predictions, y_train[b:b+batch_size, 0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = criterion(model(x_val, y_val[:, 1]), y_val[:, 0]).item()\n",
    "        val_losses.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "        pbar.set_description(f\"Loss: {val_loss:.9f}\")\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = deepcopy(model)\n",
    "    return best_model, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2b2a5d3-6e4d-459e-b3b0-3f03abf9fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 128\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1147cba6-e06a-4aec-9ed6-82d0e508fdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83606d0ae4bb4c21aba0810744c25edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_neural_operator, losses_neural_operator \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneural_operator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, x_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], batch_size):\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m:\u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m:\u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(predictions, y_train[b:b\u001b[38;5;241m+\u001b[39mbatch_size, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 51\u001b[0m, in \u001b[0;36mNeuralOperator.forward\u001b[0;34m(self, inputs, query)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, query):\n\u001b[1;32m     50\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mview(inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, query\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/neuralop/models/fno.py:253\u001b[0m, in \u001b[0;36mFNO.forward\u001b[0;34m(self, x, output_shape, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_padding\u001b[38;5;241m.\u001b[39mpad(x)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[0;32m--> 253\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfno_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_padding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_padding\u001b[38;5;241m.\u001b[39munpad(x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/neuralop/layers/fno_block.py:195\u001b[0m, in \u001b[0;36mFNOBlocks.forward\u001b[0;34m(self, x, index, output_shape)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_with_preactivation(x, index, output_shape)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_with_postactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/neuralop/layers/fno_block.py:208\u001b[0m, in \u001b[0;36mFNOBlocks.forward_with_postactivation\u001b[0;34m(self, x, index, output_shape)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstabilizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    206\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(x)\n\u001b[0;32m--> 208\u001b[0m x_fno \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     x_fno \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_norms \u001b[38;5;241m*\u001b[39m index](x_fno)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/neuralop/layers/spectral_convolution.py:459\u001b[0m, in \u001b[0;36mSpectralConv.forward\u001b[0;34m(self, x, indices, output_shape)\u001b[0m\n\u001b[1;32m    457\u001b[0m slices_x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mslice\u001b[39m(start\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39mstart\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(start, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m start \u001b[38;5;129;01min\u001b[39;00m starts[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m    458\u001b[0m slices_x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m-\u001b[39mstarts[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m starts[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)] \u001b[38;5;66;03m# The last mode already has redundant half removed\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m out_fft[slices_x] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_contract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mslices_x\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_scaling_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m output_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     mode_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;28mround\u001b[39m(s \u001b[38;5;241m*\u001b[39m r) \u001b[38;5;28;01mfor\u001b[39;00m (s, r) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(mode_sizes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_scaling_factor[indices])])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/neuralop/layers/spectral_convolution.py:46\u001b[0m, in \u001b[0;36m_contract_dense\u001b[0;34m(x, weight, separable)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum_complexhalf(eq, x, weight)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43meq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorly/backend/__init__.py:206\u001b[0m, in \u001b[0;36mBackendManager.dispatch_backend_method.<locals>.wrapped_backend_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_backend_method\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A dynamically dispatched method\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    Returns the queried method from the currently set backend\"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_THREAD_LOCAL_DATA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorly/plugins.py:77\u001b[0m, in \u001b[0;36muse_opt_einsum.<locals>.cached_einsum\u001b[0;34m(equation, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m     expression \u001b[38;5;241m=\u001b[39m oe\u001b[38;5;241m.\u001b[39mcontract_expression(equation, \u001b[38;5;241m*\u001b[39mshapes, optimize\u001b[38;5;241m=\u001b[39moptimize)\n\u001b[1;32m     75\u001b[0m     OPT_EINSUM_PATH_CACHE[key] \u001b[38;5;241m=\u001b[39m expression\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexpression\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/opt_einsum/contract.py:763\u001b[0m, in \u001b[0;36mContractExpression.__call__\u001b[0;34m(self, *arrays, **kwargs)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backends\u001b[38;5;241m.\u001b[39mhas_backend(backend) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays):\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_contract_with_conversion(ops, out, backend, evaluate_constants\u001b[38;5;241m=\u001b[39mevaluate_constants)\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_contract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate_constants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluate_constants\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    766\u001b[0m     original_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(err\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;28;01mif\u001b[39;00m err\u001b[38;5;241m.\u001b[39margs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/opt_einsum/contract.py:693\u001b[0m, in \u001b[0;36mContractExpression._contract\u001b[0;34m(self, arrays, out, backend, evaluate_constants)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The normal, core contraction.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    691\u001b[0m contraction_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_contraction_list \u001b[38;5;28;01mif\u001b[39;00m evaluate_constants \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontraction_list\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_core_contract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcontraction_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mevaluate_constants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluate_constants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/opt_einsum/contract.py:591\u001b[0m, in \u001b[0;36m_core_contract\u001b[0;34m(operands, contraction_list, backend, evaluate_constants, **einsum_kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m         einsum_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m out_array\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;66;03m# Do the contraction\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m     new_view \u001b[38;5;241m=\u001b[39m \u001b[43m_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43meinsum_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtmp_operands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meinsum_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;66;03m# Append new items and dereference what we can\u001b[39;00m\n\u001b[1;32m    594\u001b[0m operands\u001b[38;5;241m.\u001b[39mappend(new_view)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/opt_einsum/sharing.py:151\u001b[0m, in \u001b[0;36meinsum_cache_wrap.<locals>.cached_einsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(einsum)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_einsum\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m currently_sharing():\n\u001b[0;32m--> 151\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# hash modulo commutativity by computing a canonical ordering and names\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     backend \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/opt_einsum/contract.py:353\u001b[0m, in \u001b[0;36m_einsum\u001b[0;34m(*operands, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         einsum_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m->\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m parser\u001b[38;5;241m.\u001b[39mfind_output_str(einsum_str)\n\u001b[1;32m    351\u001b[0m     einsum_str \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mconvert_to_valid_einsum_chars(einsum_str)\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43meinsum_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/opt_einsum/backends/torch.py:45\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m     42\u001b[0m equation \u001b[38;5;241m=\u001b[39m convert_to_valid_einsum_chars(equation)\n\u001b[1;32m     44\u001b[0m torch, _ \u001b[38;5;241m=\u001b[39m _get_torch_and_device()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/functional.py:402\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    399\u001b[0m     _operands \u001b[38;5;241m=\u001b[39m operands[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_operands\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39meinsum(equation, operands)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/functional.py:407\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    409\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_neural_operator, losses_neural_operator = train(neural_operator, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651600c1-3ee8-48ef-b13f-8481244aa6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_neural_network, losses_neural_network = train(neural_network, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00482b-e855-4a93-a71d-65ccbac0769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cnn, losses_cnn = train(cnn, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6284315-2dec-4c82-b511-05eda0694187",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_hat_nn = best_neural_network(x_test, y_test[:, 1]).to('cpu').squeeze().detach().numpy()\n",
    "    y_hat_cnn = best_cnn(x_test, y_test[:, 1]).to('cpu').squeeze().detach().numpy()\n",
    "    y_hat_no = best_neural_operator(x_test, y_test[:, 1]).to('cpu').squeeze().detach().numpy()\n",
    "    \n",
    "y_hat_scipy = diff_interpolate(((x_test[:, 0]+mini_0)*(maxi_0-mini_0)).to('cpu'), ((x_test[:, 1]+mini_1)*(maxi_1-mini_1)).to('cpu'), y_test[:, 1].to('cpu'))\n",
    "y_hat_scipy = (y_hat_scipy-mini_0_y.item())/(maxi_0_y.item()-mini_0_y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc1371-361e-4550-8628-ffc70effa492",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0\n",
    "errors = {\"NN\": [], \"CNN\": [], \"NO\": [], \"SP\": []}\n",
    "n_samples = 10\n",
    "for j in range(n_samples):\n",
    "    i = r + j\n",
    "    fig, ax = plt.subplots(1, 5, figsize=(25, 4))\n",
    "    for a in range(4):\n",
    "        ax[a+1].plot(y_test[i, 1].to(\"cpu\"), y_test[i, 0].to(\"cpu\"), linestyle=\"--\", label=\"f'(x)\")\n",
    "    ax[0].plot(x_test[i, 1].to(\"cpu\"), x_test[i, 0].to(\"cpu\"), label='f(x)')\n",
    "    ax[1].plot(y_test[i, 1].to(\"cpu\"), y_hat_nn[i], label='nn')\n",
    "    ax[2].plot(y_test[i, 1].to(\"cpu\"), y_hat_cnn[i], label='cnn')\n",
    "    ax[3].plot(y_test[i, 1].to(\"cpu\"), y_hat_no[i], label='no')\n",
    "    ax[4].plot(y_test[i, 1, :-1].to(\"cpu\"), y_hat_scipy[i], label='sp')\n",
    "    ax[0].set_title('Input function')\n",
    "    errors[\"NN\"].append(error(y_test[i, 0].to(\"cpu\").numpy(), y_hat_nn[i]))\n",
    "    errors[\"CNN\"].append(error(y_test[i, 0].to(\"cpu\").numpy(), y_hat_cnn[i]))\n",
    "    errors[\"NO\"].append(error(y_test[i, 0].to(\"cpu\").numpy(), y_hat_no[i]))\n",
    "    errors[\"SP\"].append(error(y_test[i, 0, :-1].to(\"cpu\").numpy(), y_hat_scipy[i]))\n",
    "    ax[1].set_title(f'Neural Network\\nMSE: {errors[\"NN\"][-1]:.7f}')\n",
    "    ax[2].set_title(f'Convolutional Neural Network\\nMSE: {errors[\"CNN\"][-1]:.7f}')\n",
    "    ax[3].set_title(f'Neural Operator\\nMSE: {errors[\"NO\"][-1]:.7f}')\n",
    "    ax[4].set_title(f'Numerical differentiation by interpolation\\nMSE: {errors[\"SP\"][-1]:.7f}')\n",
    "    for a in range(5):\n",
    "        ax[a].set_ylim(-0.5, 1.5)\n",
    "        ax[a].legend()\n",
    "        ax[a].grid('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b30ce-5a9c-47af-b508-2d854abeb05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0\n",
    "errors = {\"NN\": [], \"CNN\": [], \"NO\": [], \"SP\": []}\n",
    "n_samples = x_test.shape[0]\n",
    "for j in range(n_samples):\n",
    "    i = r + j\n",
    "    errors[\"NN\"].append(error(y_test[i, 0].to(\"cpu\").numpy(), y_hat_nn[i]))\n",
    "    errors[\"CNN\"].append(error(y_test[i, 0].to(\"cpu\").numpy(), y_hat_cnn[i]))\n",
    "    errors[\"NO\"].append(error(y_test[i, 0].to(\"cpu\").numpy(), y_hat_no[i]))\n",
    "    errors[\"SP\"].append(error(y_test[i, 0, :-1].to(\"cpu\").numpy(), y_hat_scipy[i]))\n",
    "averages = {i: [np.mean(errors[i])] for i in errors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930a162-197e-40dd-a189-204b1299175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted(averages, key=lambda x: averages[x]):\n",
    "    print(f\"{i}:\\t{averages[i][0]:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b1012-c74c-4e95-8f3a-bad5ce18260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = pd.DataFrame(errors)\n",
    "averages = pd.DataFrame(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9d80a-2085-4cc6-8fec-819dc088c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(17, 10))\n",
    "sns.stripplot(data=errors, ax=ax, marker='.')\n",
    "sns.stripplot(data=averages, ax=ax, color='black', marker='o')\n",
    "for i in averages:\n",
    "    ax.plot([0, 3], [averages[i], averages[i]])\n",
    "plt.legend([\"Multi-layer Perceptron\", \"Convolutional Neural Network\", \"Neural Operator\", \"Interpolation differentiation\"])\n",
    "plt.title(\"Errors\")\n",
    "plt.grid('on')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
