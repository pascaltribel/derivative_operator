{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e51053-378c-4b5c-88ec-017c5943f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import neuralop\n",
    "import matplotlib.pyplot as plt\n",
    "import symengine as se\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc9fba6-1702-41e1-8a84-9fbe75bd3a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_transform(f, x):\n",
    "    return f((50*float(np.random.random() - 0.5)) * x + (float(np.random.random() - 0.5))) + (float(np.random.random() - 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53609f9c-9913-4c9c-bba8-af76f17aeb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_polynom(x):\n",
    "    degree = np.random.randint(1, 5)\n",
    "    expression = (float(np.random.random() - 0.5)) * x**degree\n",
    "    degree -= 1\n",
    "    while degree >= 0:\n",
    "        expression += 0.001*(float(np.random.random() - 0.5)) * x**degree\n",
    "        degree -= 1\n",
    "    return expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e734aa1-d428-46b0-b221-ffb730c68bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_log(x):\n",
    "    return (float(np.random.random() - 0.5)) * se.log(x + 1 + (float(np.random.random())))\n",
    "    \n",
    "def random_sqrt(x):\n",
    "    return (float(np.random.random() - 0.5)) * se.sqrt(x + 1 + (float(np.random.random())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c06dbe05-53e8-46c8-b98c-93310cfb425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sum(l):\n",
    "    res = []\n",
    "    for f1 in l:\n",
    "        res.append((float(np.random.random() - 0.5)) * f1 + (float(np.random.random() - 0.5)) * np.random.choice(l))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13ab77cc-b9a1-411f-8062-e70734d42b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_product(l):\n",
    "    res = []\n",
    "    for f1 in l:\n",
    "        res.append((float(np.random.random() - 0.5)) * f1 * (float(np.random.random() - 0.5)) * np.random.choice(l))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "023d37a8-c62f-45d6-b8a7-cae0bd3fae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_function(x, num_points, length):\n",
    "    maximum = 10\n",
    "    minimum = -10\n",
    "    while maximum >= 10 or minimum <= -10 or np.abs(np.max(f_eval)-np.min(f_eval)) <= 10e-2:\n",
    "        bases = np.random.choice([random_transform(se.sin, x),\n",
    "                                  random_transform(se.cos, x),\n",
    "                                  random_transform(se.sinh, x),\n",
    "                                  random_transform(se.cosh, x),\n",
    "                                  random_log(x),\n",
    "                                  random_sqrt(x),\n",
    "                                  random_polynom(x)], size=np.random.randint(1, 5))\n",
    "        f = bases[0]\n",
    "        for i in range(1, len(bases)):\n",
    "            if np.random.random() < 0.5:\n",
    "                f = random_sum([f, bases[i]])[0]\n",
    "            else:\n",
    "                f = random_product([f, bases[i]])[0]\n",
    "        df = se.diff(f, x)\n",
    "        start = np.random.random()-1\n",
    "        end = start + length\n",
    "        points = np.sort(np.random.uniform(start, end, size=num_points))\n",
    "        f_eval = np.array([float(f.subs({x: p})) for p in points]) + np.random.normal(0, 1e-2, size=num_points)\n",
    "        df_eval = np.array([float(df.subs({x: p})) for p in points])\n",
    "        maximum = np.max([np.max(f_eval), np.max(df_eval)])\n",
    "        minimum = np.min([np.min(f_eval), np.min(df_eval)])\n",
    "    return f, df, points, f_eval, df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41a270d2-e3bd-4108-9fe3-75e01e2a1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = int(2**14)\n",
    "length = 3\n",
    "num_points = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f62d6046-34bb-47a8-b616-32d240300206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 16384 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824a1a5e188849a1b5d0b9f3ae932013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'cp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m     X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(functions_eval, points)))\n\u001b[1;32m     22\u001b[0m     Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(derivatives_eval)\n\u001b[0;32m---> 23\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mcp\u001b[49m\u001b[38;5;241m.\u001b[39masnumpy(X))\n\u001b[1;32m     24\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, cp\u001b[38;5;241m.\u001b[39masnumpy(Y))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cp' is not defined"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def generate_sample(x, num_points, length):\n",
    "    f, df, p, f_e, df_e = random_function(x, num_points, length)\n",
    "    return f, df, p, f_e, df_e\n",
    "\n",
    "GENERATING = True\n",
    "\n",
    "if GENERATING:\n",
    "    print(\"Generating\", total_size, \"samples\")\n",
    "    x = se.Symbol('x')\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(delayed(generate_sample)(x, num_points, length) for _ in tqdm(range(total_size)))\n",
    "\n",
    "    functions, derivatives, points, functions_eval, derivatives_eval = zip(*results)\n",
    "\n",
    "    X = np.array(list(zip(functions_eval, points)))\n",
    "    Y = np.array(derivatives_eval)\n",
    "    np.save(\"X.npy\", X)\n",
    "    np.save(\"Y.npy\", Y)\n",
    "else:\n",
    "    X, Y = np.load(\"X.npy\"), np.load(\"Y.npy\")\n",
    "    total_size, _, num_points = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "845a079e-a6e5-4165-9951-5df1b5c010d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(X.shape[0]*0.5)\n",
    "val_size = int(X.shape[0]*0.25)\n",
    "test_size = X.shape[0] - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a43ce650-a100-4240-b2bc-623ebca3b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = torch.tensor(X[:train_size], dtype=torch.float32), torch.tensor(Y[:train_size], dtype=torch.float32)\n",
    "x_val, y_val = torch.tensor(X[train_size:train_size+val_size], dtype=torch.float32), torch.tensor(Y[train_size:train_size+val_size], dtype=torch.float32)\n",
    "x_test, y_test = torch.tensor(X[train_size+val_size:], dtype=torch.float32), torch.tensor(Y[train_size+val_size:], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a77ca64-90f1-46ee-845c-3423e7743154",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi_0 = x_train[:, 0].max()\n",
    "mini_0 = x_train[:, 0].min()\n",
    "maxi_1 = x_train[:, 1].max()\n",
    "mini_1 = x_train[:, 1].min()\n",
    "\n",
    "x_train[:, 0] = (x_train[:, 0]-mini_0)/(maxi_0-mini_0)\n",
    "x_train[:, 1] = (x_train[:, 1]-mini_1)/(maxi_1-mini_1)\n",
    "\n",
    "x_val[:, 0] = (x_val[:, 0]-mini_0)/(maxi_0-mini_0)\n",
    "x_val[:, 1] = (x_val[:, 1]-mini_1)/(maxi_1-mini_1)\n",
    "\n",
    "x_test[:, 0] = (x_test[:, 0]-mini_0)/(maxi_0-mini_0)\n",
    "x_test[:, 1] = (x_test[:, 1]-mini_1)/(maxi_1-mini_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fd73a9d-4590-4e3a-8f7f-5ced089ad4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_y, min_y = torch.max(y_train), torch.min(y_train)\n",
    "y_train = (y_train-min_y)/(max_y-min_y)\n",
    "y_val = (y_val-min_y)/(max_y-min_y)\n",
    "y_test = (y_test-min_y)/(max_y-min_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "900f76a1-4447-4bec-8572-1cb798851f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 128\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4482eef6-84da-446e-a66b-bb806f68dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1459abc7-909d-4181-a9ec-e2e83ddf8475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3584\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = nn.Linear(shape*2, 512)\n",
    "        self.l2 = nn.Linear(512, 256)\n",
    "        self.l3 = nn.Linear(256, 512)\n",
    "        self.l4 = nn.Linear(512, shape)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.l1(inputs.reshape((inputs.shape[0], -1))))\n",
    "        x = self.relu(self.l2(x))\n",
    "        x = self.relu(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model1 = Model(num_points).to(device)\n",
    "best_model1 = deepcopy(model1)\n",
    "print(\"Number of parameters:\", np.sum([len(i) for i in model1.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efad717c-631b-4d24-b662-968302530264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398f8aefdb694f55b01acfa5867f99ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /opt/conda/conda-bld/pytorch_1720538440907/work/aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model1.parameters(), lr=0.0005)\n",
    "losses_nn = []\n",
    "for epoch in (pbar:=tqdm(range(n_epochs))):\n",
    "    model1.train()\n",
    "    optimizer.zero_grad()\n",
    "    for b in range(0, x_train.shape[0], batch_size):\n",
    "        predictions = model1(x_train[b: b+batch_size].to(device)).to('cpu')\n",
    "        loss = criterion(predictions, y_train[b: b+batch_size])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model1.eval()\n",
    "    losses_nn.append(criterion(model1(x_val.to(device)).to('cpu'), y_val).item())\n",
    "    pbar.set_description(f\"{losses_nn[-1]}\")\n",
    "    if losses_nn[-1] == min(losses_nn):\n",
    "        best_model1 = deepcopy(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07d9e3bd-b7eb-4fb8-a121-241826a672e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test loss: 0.009668410755693913\n",
      "Best test loss: 0.009602050296962261\n"
     ]
    }
   ],
   "source": [
    "print(\"Final test loss:\", criterion(model1(x_test.to(device)).to('cpu'), y_test).item())\n",
    "print(\"Best test loss:\", criterion(best_model1(x_test.to(device)).to('cpu'), y_test).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f967ad3-3c21-4e42-8c33-611e8c3492f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1178\n"
     ]
    }
   ],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            neuralop.models.FNO1d(n_modes_height=128, n_layers=8, in_channels=2, out_channels=1, hidden_channels=8),\n",
    "            #neuralop.models.UNO(in_channels=2, out_channels=1, hidden_channels=4, uno_out_channels=[4,4,4,4], uno_n_modes=[[64],[64],[64],[64]], \n",
    "            #                    uno_scalings=[[1],[1],[1],[1]])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model2 = Model2().to(device)\n",
    "best_model2 = deepcopy(model2)\n",
    "print(\"Number of parameters:\", np.sum([len(i) for i in model2.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d54f9308-de68-4655-b85f-8be8e446398a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d229fde22447f79140940b9225d44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 10.92 GiB of which 322.69 MiB is free. Process 737880 has 2.26 GiB memory in use. Process 988319 has 5.51 GiB memory in use. Process 1042719 has 2.82 GiB memory in use. Of the allocated memory 2.04 GiB is allocated by PyTorch, and 22.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m model2\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 13\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(criterion(\u001b[43mmodel2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(), y_test)\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     14\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mmin\u001b[39m(losses):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m, in \u001b[0;36mModel2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/neuralop/models/fno.py:247\u001b[0m, in \u001b[0;36mFNO.forward\u001b[0;34m(self, x, output_shape, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_shape, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    245\u001b[0m     output_shape \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m [output_shape]\n\u001b[0;32m--> 247\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlifting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_padding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    250\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_padding\u001b[38;5;241m.\u001b[39mpad(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/neuralop/layers/mlp.py:64\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m fc(x)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnon_linearity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout[i](x)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 10.92 GiB of which 322.69 MiB is free. Process 737880 has 2.26 GiB memory in use. Process 988319 has 5.51 GiB memory in use. Process 1042719 has 2.82 GiB memory in use. Of the allocated memory 2.04 GiB is allocated by PyTorch, and 22.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model2.parameters(), lr=0.0005)\n",
    "losses = []\n",
    "for epoch in (pbar:=tqdm(range(n_epochs))):\n",
    "    model2.train()\n",
    "    optimizer.zero_grad()\n",
    "    for b in range(0, x_train.shape[0], batch_size):\n",
    "        predictions = model2(x_train[b: b+batch_size].unsqueeze(1).to(device)).to('cpu')\n",
    "        loss = criterion(predictions.squeeze(), y_train[b: b+batch_size])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model2.eval()\n",
    "    losses.append(criterion(model2(x_test.unsqueeze(1).to(device)).to('cpu').squeeze(), y_test).item())\n",
    "    pbar.set_description(f\"{losses[-1]}\")\n",
    "    if losses[-1] == min(losses):\n",
    "        best_model2 = deepcopy(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f2c5a5-18a7-44ba-a864-ca6f4297a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final test loss:\", criterion(model2.to(device)(x_test).squeeze(), y_test).item())\n",
    "print(\"Best test loss:\", criterion(best_model2.to(device)(x_test).squeeze(), y_test).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc1371-361e-4550-8628-ffc70effa492",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.eval()\n",
    "model2.eval()\n",
    "r = 4\n",
    "for i in [r, r+1]:\n",
    "    with torch.no_grad():\n",
    "        y_hat_nn, y_hat_no = best_model1(x_test[i:i+1].to(device)).to('cpu').squeeze(), best_model2(x_test[i:i+1]).squeeze()\n",
    "    y_hat_np = np.diff((x_test[i, 0]+mini_0)*(maxi_0-mini_0))/(length/num_points)\n",
    "    y_hat_np = (y_hat_np-min_y.item())/(max_y.item()-min_y.item())\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    ax[0].plot(x_test[i, 1], y_test[i], linestyle=\"--\", label='truth')\n",
    "    ax[1].plot(x_test[i, 1], y_test[i], linestyle=\"--\", label='truth')\n",
    "    ax[2].plot(x_test[i, 1], y_test[i], linestyle=\"--\", label='truth')\n",
    "    ax[0].plot(x_test[i, 1], y_hat_nn, label='nn')\n",
    "    ax[1].plot(x_test[i, 1], y_hat_no, label='no')\n",
    "    ax[2].plot(x_test[i, 1, 1:], y_hat_np, label='np')\n",
    "    ax[0].grid('on'), ax[1].grid('on'), ax[2].grid('on')\n",
    "    ax[0].set_title('Neural Network'), ax[1].set_title('Neural Operator'), ax[2].set_title('Numerical differentiation')\n",
    "    ax[0].legend(), ax[1].legend(), ax[2].legend()\n",
    "    plt.show()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b083a5-5c9e-40b2-9181-8ac2794b13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(losses_nn, label='nn')\n",
    "plt.plot(losses, label='no')\n",
    "plt.title(f\"Neural Network ({sum([sum(p.shape) for p in model1.parameters()])} parameters): {np.mean(losses_nn[-20:]) : .7f}\\n\" +\n",
    "          f\"Neural Operator ({sum([sum(p.shape) for p in model2.parameters()])} parameters): {np.mean(losses[-20:]) : .7f}\")\n",
    "plt.yscale('log')\n",
    "plt.grid('on')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9865bd84-e2e5-4976-96d5-e3d1e47cfe81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
